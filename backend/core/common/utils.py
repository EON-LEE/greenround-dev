import os
import uuid
import shutil
import logging
import asyncio
from typing import Dict, Any, Optional
import cv2
from pathlib import Path
from google.cloud import storage
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ìŠ¤í† ë¦¬ì§€ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).parent.parent.parent
STORAGE_DIR = BASE_DIR / "storage"
UPLOADS_DIR = STORAGE_DIR / "uploads"
HIGHLIGHTS_DIR = STORAGE_DIR / "highlights"
SEQUENCES_DIR = STORAGE_DIR / "sequences"
BALL_TRACKS_DIR = STORAGE_DIR / "ball_tracks"
TEMP_DIR = STORAGE_DIR / "temp"

# ë””ë ‰í† ë¦¬ ìƒì„±
for dir_path in [UPLOADS_DIR, HIGHLIGHTS_DIR, SEQUENCES_DIR, BALL_TRACKS_DIR, TEMP_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# ê¸€ë¡œë²Œ íƒœìŠ¤í¬ ìƒíƒœ ì €ì¥ì†Œ
task_status: Dict[str, Dict[str, Any]] = {}

# --- GCS ì„¤ì • ---
GCS_BUCKET_NAME = os.getenv("GCS_BUCKET_NAME")
if not GCS_BUCKET_NAME:
    logger.warning("GCS_BUCKET_NAME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GCS ì—…ë¡œë“œê°€ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

def generate_file_id() -> str:
    """ê³ ìœ í•œ íŒŒì¼ ID ìƒì„±"""
    return str(uuid.uuid4())

def generate_task_id(prefix: str = "task") -> str:
    """ê³ ìœ í•œ íƒœìŠ¤í¬ ID ìƒì„±"""
    return f"{prefix}_{str(uuid.uuid4())[:8]}"

def get_service_base_url() -> str:
    """ì„œë¹„ìŠ¤ ë² ì´ìŠ¤ URL ë°˜í™˜ (í™˜ê²½ì— ë”°ë¼ ë™ì  ì„¤ì •)"""
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ BASE_URLì„ ê°€ì ¸ì˜¤ê±°ë‚˜, ê¸°ë³¸ê°’ ì‚¬ìš©
    base_url = os.getenv("SERVICE_BASE_URL", "https://golf-analyzer-backend-984220723638.asia-northeast3.run.app")
    return base_url.rstrip('/')

def generate_predictable_urls(task_id: str, content_type: str) -> dict:
    """ì˜ˆì¸¡ ê°€ëŠ¥í•œ URLë“¤ì„ ë¯¸ë¦¬ ìƒì„±"""
    base_url = get_service_base_url()
    
    # content_type: "highlights", "sequences", "ball_tracks"
    urls = {
        "download": f"{base_url}/api/results/{content_type}/{task_id}",
        "stream": f"{base_url}/api/results/{content_type}/{task_id}/stream",
        "status": f"{base_url}/api/status/{task_id}"
    }
    
    # ì´ë¯¸ì§€ íƒ€ì…ì¸ ê²½ìš° í™•ì¥ì ì¶”ê°€
    if content_type == "sequences":
        urls["download"] += ".png"
        urls["stream"] = urls["download"]  # ì´ë¯¸ì§€ëŠ” ìŠ¤íŠ¸ë¦¬ë°ê³¼ ë‹¤ìš´ë¡œë“œê°€ ë™ì¼
    else:
        urls["download"] += ".mp4"
    
    return urls

def get_file_path(file_id: str, directory: str = "uploads") -> Path:
    """íŒŒì¼ IDë¡œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
    dir_map = {
        "uploads": UPLOADS_DIR,
        "highlights": HIGHLIGHTS_DIR,
        "sequences": SEQUENCES_DIR,
        "ball_tracks": BALL_TRACKS_DIR,
        "temp": TEMP_DIR
    }
    return dir_map.get(directory, UPLOADS_DIR) / file_id

def validate_video_file(file_path: Path) -> bool:
    """ë¹„ë””ì˜¤ íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
    try:
        if not file_path.exists():
            return False
            
        cap = cv2.VideoCapture(str(file_path))
        if not cap.isOpened():
            return False
            
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        cap.release()
        
        # ìµœì†Œ ìš”êµ¬ì‚¬í•­ í™•ì¸
        if frame_count < 30 or fps < 10 or width < 100 or height < 100:
            return False
            
        return True
        
    except Exception as e:
        logger.error(f"Video validation error: {str(e)}")
        return False

def get_video_info(file_path: Path) -> Dict[str, Any]:
    """ë¹„ë””ì˜¤ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
    try:
        cap = cv2.VideoCapture(str(file_path))
        if not cap.isOpened():
            raise ValueError("Cannot open video file")
            
        info = {
            "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            "fps": cap.get(cv2.CAP_PROP_FPS),
            "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            "duration": 0
        }
        
        if info["fps"] > 0:
            info["duration"] = info["frame_count"] / info["fps"]
        
        cap.release()
        return info
        
    except Exception as e:
        logger.error(f"Error getting video info: {str(e)}")
        return {}

def update_task_status(task_id: str, status: str, progress: int = 0, 
                      message: str = None, result_data: dict = None):
    """íƒœìŠ¤í¬ ìƒíƒœ ì—…ë°ì´íŠ¸ (Firestore ë™ê¸°í™” í¬í•¨)"""
    # ğŸ”„ ê¸°ì¡´ ë©”ëª¨ë¦¬ ì €ì¥ (100% ë™ì¼)
    task_status[task_id] = {
        "status": status,
        "progress": progress,
        "message": message,
        "result_data": result_data or {}
    }
    logger.info(f"Task {task_id} updated: {status} ({progress}%)")
    
    # ğŸ†• Firestore ë™ê¸°í™” (ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ ê¸°ëŠ¥ ì •ìƒ ë™ì‘)
    try:
        from .firestore_sync import safe_sync_to_firestore
        safe_sync_to_firestore(task_id, task_status[task_id])
    except Exception as e:
        logger.warning(f"Firestore ë™ê¸°í™” ì‹¤íŒ¨ (ê¸°ì¡´ ê¸°ëŠ¥ì€ ì •ìƒ): {e}")

def get_task_status(task_id: str) -> Dict[str, Any]:
    """íƒœìŠ¤í¬ ìƒíƒœ ì¡°íšŒ (Firestore í´ë°± í¬í•¨)"""
    # ğŸ”„ ê¸°ì¡´ ë©”ëª¨ë¦¬ ì¡°íšŒ (ìš°ì„ ìˆœìœ„ 1)
    if task_id in task_status:
        return task_status[task_id]
    
    # ğŸ†• Firestoreì—ì„œ ë³µêµ¬ ì‹œë„ (Cloud Run ì¬ì‹œì‘ ëŒ€ì‘)
    try:
        from .firestore_sync import safe_get_from_firestore
        firestore_data = safe_get_from_firestore(task_id)
        if firestore_data:
            # ë©”ëª¨ë¦¬ì— ë³µì›
            task_status[task_id] = firestore_data
            logger.info(f"ğŸ“¥ Firestoreì—ì„œ ìƒíƒœ ë³µêµ¬: {task_id}")
            return firestore_data
    except Exception as e:
        logger.warning(f"Firestore ë³µêµ¬ ì‹¤íŒ¨ (ê¸°ì¡´ ë¡œì§ ì‚¬ìš©): {e}")
    
    # ğŸ”„ ê¸°ì¡´ ê¸°ë³¸ê°’ ë°˜í™˜ (100% ë™ì¼)
    return {
        "status": "not_found",
        "progress": 0,
        "message": "Task not found"
    }

def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
    try:
        for file_path in TEMP_DIR.glob("*"):
            if file_path.is_file():
                file_path.unlink()
        logger.info("Temporary files cleaned up")
    except Exception as e:
        logger.error(f"Error cleaning temp files: {str(e)}")

def cleanup_old_files(max_age_hours: int = 24):
    """ì˜¤ë˜ëœ íŒŒì¼ë“¤ ì •ë¦¬ (Firestore ì •ë¦¬ í¬í•¨)"""
    import time
    
    try:
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        for directory in [UPLOADS_DIR, HIGHLIGHTS_DIR, SEQUENCES_DIR, BALL_TRACKS_DIR]:
            for file_path in directory.glob("*"):
                if file_path.is_file():
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > max_age_seconds:
                        file_path.unlink()
                        logger.info(f"Cleaned old file: {file_path.name}")
                        
    except Exception as e:
        logger.error(f"Error cleaning old files: {str(e)}")
    
    # ğŸ†• Firestore ì •ë¦¬ (ì‹¤íŒ¨í•´ë„ ê¸°ì¡´ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ)
    try:
        from .firestore_sync import cleanup_old_firestore_tasks
        cleanup_old_firestore_tasks(max_age_hours)
    except Exception as e:
        logger.warning(f"Firestore ì •ë¦¬ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

async def run_background_task(task_func, *args, **kwargs):
    """ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹¤í–‰"""
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, task_func, *args, **kwargs)
    except Exception as e:
        logger.error(f"Background task error: {str(e)}")
        raise 

def upload_to_gcs_and_get_public_url(local_file_path: Path, destination_blob_name: str) -> str:
    """
    ë¡œì»¬ íŒŒì¼ì„ GCSì— ì—…ë¡œë“œí•˜ê³  Signed URL(1ì‹œê°„ ìœ íš¨)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ 'GOOGLE_APPLICATION_CREDENTIALS' í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    try:
        if not GCS_BUCKET_NAME:
            logger.error("GCS_BUCKET_NAME í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ íŒŒì¼ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
            # GCS ì—…ë¡œë“œ ëŒ€ì‹  ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
            return f"file://{local_file_path}"
        
        from datetime import datetime, timedelta
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(GCS_BUCKET_NAME)
        blob = bucket.blob(destination_blob_name)

        blob.upload_from_filename(str(local_file_path))

        # ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if local_file_path.exists():
            os.remove(local_file_path)

        # Signed URL ìƒì„± (1ì‹œê°„ ìœ íš¨)
        signed_url = blob.generate_signed_url(
            version="v4",
            expiration=datetime.utcnow() + timedelta(hours=1),
            method="GET"
        )

        logger.info(f"'{local_file_path}'ë¥¼ GCS ë²„í‚· '{GCS_BUCKET_NAME}'ì— '{destination_blob_name}'(ìœ¼)ë¡œ ì—…ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        logger.info(f"Signed URL ìƒì„±ë¨ (1ì‹œê°„ ìœ íš¨): {signed_url[:100]}...")
        return signed_url
        
    except Exception as e:
        logger.error(f"GCS ì—…ë¡œë“œ ì‹¤íŒ¨: {e}", exc_info=True)
        # ì—…ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ íŒŒì¼ ê²½ë¡œ ë°˜í™˜ (fallback)
        logger.warning(f"GCS ì—…ë¡œë“œ ì‹¤íŒ¨ë¡œ ë¡œì»¬ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤: {local_file_path}")
        return f"file://{local_file_path}" 